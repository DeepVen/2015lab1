{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHFFfgf7iCNA",
        "outputId": "1ce8360a-0c28-40c4-cd01-d64d54948b33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.300-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.0-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain)\n",
            "  Downloading langsmith-0.0.40-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.300 langsmith-0.0.40 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blOh6h-riwY2",
        "outputId": "80f17036-ab38-4718-d3bd-a47f0e513bf4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.17.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings #for using HugginFace models\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "zCNLItuMiLrR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
      ],
      "metadata": {
        "id": "3cthK4PRic6o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":1, \"max_length\":1000000})"
      ],
      "metadata": {
        "id": "i-78-pe5iXAU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DRQgKOv4mLPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.:     can google colab save changes to github\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cA30lECXjo7h",
        "outputId": "a90bc22a-b3c2-4271-8300-d8d6f275d188"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Google Collab does not support Github.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.:    who is venkat rajagopalan\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-e_XDQgWkxBR",
        "outputId": "dd5e246b-708f-4f52-8964-6cf9992efd4d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Venkat Rajagopalan is an Indian film actor.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: Which mobile is best?\n",
        "\n",
        "Answer: \"\"\""
      ],
      "metadata": {
        "id": "lKldeUDplMvl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fXvl5mOViXjJ",
        "outputId": "a0c9910b-fd9f-4999-ef72-003d84f5d251"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-_CwWKZlTy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt template poc"
      ],
      "metadata": {
        "id": "aqf5EwACmMJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n"
      ],
      "metadata": {
        "id": "WiUvo8NCmPZb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template\n",
        ")"
      ],
      "metadata": {
        "id": "aMPA9TvVmdGl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    prompt_template.format(\n",
        "        query=\"Which libraries and model providers offer LLMs?\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlVMtRCrmtRb",
        "outputId": "359891dc-36ae-4bbf-a849-b439cfc38839"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question based on the context below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
            "Their superior performance over smaller models has made them incredibly\n",
            "useful for developers building NLP enabled applications. These models\n",
            "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
            "using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "Question: Which libraries and model providers offer LLMs?\n",
            "\n",
            "Answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\n",
        "    prompt_template.format(\n",
        "        query=\"Which libraries and model providers offer LLMs?\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3-RKFx8Gm3Ol",
        "outputId": "1e13521e-18d1-4b3d-b189-bc3ad1e32c54"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hugging Face's transformers library, via OpenAI using the \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DtYAlqkOm4zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
        "The assistant is typically sarcastic and witty, producing creative\n",
        "and funny responses to the users questions. Here are some examples:\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\""
      ],
      "metadata": {
        "id": "-jZbYCconKhQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot learning example"
      ],
      "metadata": {
        "id": "tVTSsDLLnAyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GBKUn6wNnSxm",
        "outputId": "ec60b6ae-d4a1-4d29-c47e-74c2ae24b9fe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It's to find your purpose and be happy.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples:\n",
        "\n",
        "User: How are you?\n",
        "AI: I can't complain but sometimes I still do.\n",
        "\n",
        "User: What time is it?\n",
        "AI: It's time to get a watch.\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n"
      ],
      "metadata": {
        "id": "Pcr5NRhbnU5z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "a-tFUB6-nd4l",
        "outputId": "e6afa927-db2a-4e2d-a7da-af7a2a98cd80"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It's to eat, sleep, and repeat.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FewShotPromptTemplate"
      ],
      "metadata": {
        "id": "zI5Gn2hcwMRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create a example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "_inOcrqBneh8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the meaning of life?\"\n",
        "\n",
        "print(few_shot_prompt_template.format(query=query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l_5-2dVydYL",
        "outputId": "0aac928b-2b1b-45a5-e5ac-f262c8ae9876"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(few_shot_prompt_template.format(query=query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "P_6VQOjbzhqa",
        "outputId": "19f90c93-c4f5-41c9-892d-043236c06ef6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It's to eat, sleep, and repeat.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVcUJJSizj8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Length based example selector"
      ],
      "metadata": {
        "id": "HVciscMYztQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=20  # this sets the max length that examples should be\n",
        ")"
      ],
      "metadata": {
        "id": "Y9c4agLGzx2M"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now create the few shot prompt template\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,  # use example_selector instead of examples\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "D5MwDobU0I6x"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_prompt_template.format(query=\"How do birds fly?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8u80z4J0UeD",
        "outputId": "11e32099-4bf9-4a1a-e16f-5524aab05495"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: How do birds fly?\n",
            "AI: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(dynamic_prompt_template.format(query=\"How do birds fly?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tVX9CN4W0asv",
        "outputId": "c0859bc7-3dd4-4bff-dac3-14d9c76d95da"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'They fly by gliding.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(llm)"
      ],
      "metadata": {
        "id": "y990Ec074uXf",
        "outputId": "46d83b35-824e-4a08-a64b-6a89064a9827",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on HuggingFaceHub in module langchain.llms.huggingface_hub object:\n",
            "\n",
            "class HuggingFaceHub(langchain.llms.base.LLM)\n",
            " |  HuggingFaceHub(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, repo_id: str = 'gpt2', task: Optional[str] = None, model_kwargs: Optional[dict] = None, huggingfacehub_api_token: Optional[str] = None) -> None\n",
            " |  \n",
            " |  HuggingFaceHub  models.\n",
            " |  \n",
            " |  To use, you should have the ``huggingface_hub`` python package installed, and the\n",
            " |  environment variable ``HUGGINGFACEHUB_API_TOKEN`` set with your API token, or pass\n",
            " |  it as a named parameter to the constructor.\n",
            " |  \n",
            " |  Only supports `text-generation`, `text2text-generation` and `summarization` for now.\n",
            " |  \n",
            " |  Example:\n",
            " |      .. code-block:: python\n",
            " |  \n",
            " |          from langchain.llms import HuggingFaceHub\n",
            " |          hf = HuggingFaceHub(repo_id=\"gpt2\", huggingfacehub_api_token=\"my-api-key\")\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      HuggingFaceHub\n",
            " |      langchain.llms.base.LLM\n",
            " |      langchain.llms.base.BaseLLM\n",
            " |      langchain.schema.language_model.BaseLanguageModel\n",
            " |      langchain.load.serializable.Serializable\n",
            " |      pydantic.main.BaseModel\n",
            " |      pydantic.utils.Representation\n",
            " |      langchain.schema.runnable.base.Runnable\n",
            " |      typing.Generic\n",
            " |      abc.ABC\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
            " |      Validate that api key and python package exists in environment.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  Config = <class 'langchain.llms.huggingface_hub.HuggingFaceHub.Config'...\n",
            " |      Configuration for this pydantic object.\n",
            " |  \n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  __annotations__ = {'client': typing.Any, 'huggingfacehub_api_token': t...\n",
            " |  \n",
            " |  __class_vars__ = set()\n",
            " |  \n",
            " |  __config__ = <class 'langchain.llms.huggingface_hub.Config'>\n",
            " |  \n",
            " |  __custom_root_type__ = False\n",
            " |  \n",
            " |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
            " |  \n",
            " |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
            " |  \n",
            " |  __hash__ = None\n",
            " |  \n",
            " |  __include_fields__ = None\n",
            " |  \n",
            " |  __parameters__ = ()\n",
            " |  \n",
            " |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
            " |  \n",
            " |  __pre_root_validators__ = []\n",
            " |  \n",
            " |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
            " |  \n",
            " |  __schema_cache__ = {}\n",
            " |  \n",
            " |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...fac...\n",
            " |  \n",
            " |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain.llms.base.BaseLLM:\n",
            " |  \n",
            " |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      Check Cache and run the LLM on the given prompt and input.\n",
            " |  \n",
            " |  __str__(self) -> 'str'\n",
            " |      Get a string representation of the object for printing.\n",
            " |  \n",
            " |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
            " |      Default implementation of abatch, which calls ainvoke N times.\n",
            " |      Subclasses should override this method if they can batch more efficiently.\n",
            " |  \n",
            " |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, run_name: 'Optional[Union[str, List[str]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Run the LLM on the given prompt and input.\n",
            " |  \n",
            " |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Asynchronously pass a sequence of prompts and return model generations.\n",
            " |      \n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |      \n",
            " |      Use this method when you want to:\n",
            " |          1. take advantage of batched calls,\n",
            " |          2. need more output from the model than just the top generated value,\n",
            " |          3. are building chains that are agnostic to the underlying language model\n",
            " |              type (e.g., pure text completion models vs chat models).\n",
            " |      \n",
            " |      Args:\n",
            " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
            " |              converted to match the format of any language model (string for pure\n",
            " |              text generation models and BaseMessages for chat models).\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |              prompt and additional model provider-specific output.\n",
            " |  \n",
            " |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
            " |      Subclasses should override this method if they can run asynchronously.\n",
            " |  \n",
            " |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      Asynchronously pass a string to the model and return a string prediction.\n",
            " |      \n",
            " |      Use this method when calling pure text generation models and only the top\n",
            " |          candidate generation is needed.\n",
            " |      \n",
            " |      Args:\n",
            " |          text: String input to pass to the model.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Top model prediction as a string.\n",
            " |  \n",
            " |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      Asynchronously pass messages to the model and return a message prediction.\n",
            " |      \n",
            " |      Use this method when calling chat models and only the top\n",
            " |          candidate generation is needed.\n",
            " |      \n",
            " |      Args:\n",
            " |          messages: A sequence of chat messages corresponding to a single model input.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Top model prediction as a message.\n",
            " |  \n",
            " |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
            " |      Default implementation of astream, which calls ainvoke.\n",
            " |      Subclasses should override this method if they support streaming output.\n",
            " |  \n",
            " |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
            " |      Default implementation of batch, which calls invoke N times.\n",
            " |      Subclasses should override this method if they can batch more efficiently.\n",
            " |  \n",
            " |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
            " |      Return a dictionary of the LLM.\n",
            " |  \n",
            " |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, run_name: 'Optional[Union[str, List[str]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Run the LLM on the given prompt and input.\n",
            " |  \n",
            " |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Pass a sequence of prompts to the model and return model generations.\n",
            " |      \n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |      \n",
            " |      Use this method when you want to:\n",
            " |          1. take advantage of batched calls,\n",
            " |          2. need more output from the model than just the top generated value,\n",
            " |          3. are building chains that are agnostic to the underlying language model\n",
            " |              type (e.g., pure text completion models vs chat models).\n",
            " |      \n",
            " |      Args:\n",
            " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
            " |              converted to match the format of any language model (string for pure\n",
            " |              text generation models and BaseMessages for chat models).\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |              prompt and additional model provider-specific output.\n",
            " |  \n",
            " |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |  \n",
            " |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      Pass a single string input to the model and return a string prediction.\n",
            " |      \n",
            " |       Use this method when passing in raw text. If you want to pass in specific\n",
            " |          types of chat messages, use predict_messages.\n",
            " |      \n",
            " |      Args:\n",
            " |          text: String input to pass to the model.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Top model prediction as a string.\n",
            " |  \n",
            " |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      Pass a message sequence to the model and return a message prediction.\n",
            " |      \n",
            " |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
            " |          use predict.\n",
            " |      \n",
            " |      Args:\n",
            " |          messages: A sequence of chat messages corresponding to a single model input.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Top model prediction as a message.\n",
            " |  \n",
            " |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
            " |      Save the LLM.\n",
            " |      \n",
            " |      Args:\n",
            " |          file_path: Path to file to save the LLM to.\n",
            " |      \n",
            " |      Example:\n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          llm.save(file_path=\"path/llm.yaml\")\n",
            " |  \n",
            " |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
            " |      Default implementation of stream, which calls invoke.\n",
            " |      Subclasses should override this method if they support streaming output.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
            " |  \n",
            " |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
            " |      Raise deprecation warning if callback_manager is used.\n",
            " |  \n",
            " |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
            " |      If verbose is None, set it.\n",
            " |      \n",
            " |      This allows users to pass in None as verbose to access the global setting.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
            " |  \n",
            " |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
            " |  \n",
            " |  get_num_tokens(self, text: 'str') -> 'int'\n",
            " |      Get the number of tokens present in the text.\n",
            " |      \n",
            " |      Useful for checking if an input will fit in a model's context window.\n",
            " |      \n",
            " |      Args:\n",
            " |          text: The string input to tokenize.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The integer number of tokens in the text.\n",
            " |  \n",
            " |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
            " |      Get the number of tokens in the messages.\n",
            " |      \n",
            " |      Useful for checking if an input will fit in a model's context window.\n",
            " |      \n",
            " |      Args:\n",
            " |          messages: The message inputs to tokenize.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The sum of the number of tokens across the messages.\n",
            " |  \n",
            " |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
            " |      Return the ordered ids of the tokens in a text.\n",
            " |      \n",
            " |      Args:\n",
            " |          text: The string input to tokenize.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of ids corresponding to the tokens in the text, in order they occur\n",
            " |              in the text.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain.load.serializable.Serializable:\n",
            " |  \n",
            " |  __init__(self, **kwargs: Any) -> None\n",
            " |      Create a new model by parsing and validating input data from keyword arguments.\n",
            " |      \n",
            " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
            " |  \n",
            " |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            " |  \n",
            " |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
            " |  \n",
            " |  lc_attributes\n",
            " |      Return a list of attribute names that should be included in the\n",
            " |      serialized kwargs. These attributes must be accepted by the\n",
            " |      constructor.\n",
            " |  \n",
            " |  lc_namespace\n",
            " |      Return the namespace of the langchain object.\n",
            " |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
            " |  \n",
            " |  lc_secrets\n",
            " |      Return a map of constructor argument names to secret ids.\n",
            " |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
            " |  \n",
            " |  lc_serializable\n",
            " |      Return whether or not the class is serializable.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __eq__(self, other: Any) -> bool\n",
            " |  \n",
            " |  __getstate__(self) -> 'DictAny'\n",
            " |  \n",
            " |  __iter__(self) -> 'TupleGenerator'\n",
            " |      so `dict(model)` works\n",
            " |  \n",
            " |  __repr_args__(self) -> 'ReprArgs'\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |  \n",
            " |  __setstate__(self, state: 'DictAny') -> None\n",
            " |  \n",
            " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
            " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
            " |      \n",
            " |      :param include: fields to include in new model\n",
            " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
            " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
            " |          the new model: you should trust this data\n",
            " |      :param deep: set to `True` to make a deep copy of the model\n",
            " |      :return: new model instance\n",
            " |  \n",
            " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
            " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
            " |      \n",
            " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
            " |  \n",
            " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
            " |      Same as update_forward_refs but will not raise exception\n",
            " |      when forward references are not defined.\n",
            " |  \n",
            " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
            " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
            " |      Default values are respected, but no other validation is performed.\n",
            " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
            " |  \n",
            " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
            " |  \n",
            " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
            " |  \n",
            " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
            " |  \n",
            " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
            " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
            " |  \n",
            " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __fields_set__\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.utils.Representation:\n",
            " |  \n",
            " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
            " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
            " |  \n",
            " |  __repr__(self) -> 'unicode'\n",
            " |  \n",
            " |  __repr_name__(self) -> 'unicode'\n",
            " |      Name of the instance's class, used in __repr__.\n",
            " |  \n",
            " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
            " |  \n",
            " |  __rich_repr__(self) -> 'RichReprResult'\n",
            " |      Get fields for Rich library\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
            " |  \n",
            " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
            " |      Return self|value.\n",
            " |  \n",
            " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
            " |      Return value|self.\n",
            " |  \n",
            " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[RunLogPatch]'\n",
            " |      Stream all output from a runnable, as reported to the callback system.\n",
            " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
            " |      \n",
            " |      Output is streamed as Log objects, which include a list of\n",
            " |      jsonpatch ops that describe how the state of the run has changed in each\n",
            " |      step, and the final state of the run.\n",
            " |      \n",
            " |      The jsonpatch ops can be applied in order to construct state.\n",
            " |  \n",
            " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
            " |      Default implementation of atransform, which buffers input and calls astream.\n",
            " |      Subclasses should override this method if they can start producing output while\n",
            " |      input is still being generated.\n",
            " |  \n",
            " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
            " |      Bind arguments to a Runnable, returning a new Runnable.\n",
            " |  \n",
            " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
            " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
            " |      by calling invoke() with each input.\n",
            " |  \n",
            " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
            " |      Default implementation of transform, which buffers input and then calls stream.\n",
            " |      Subclasses should override this method if they can start producing output while\n",
            " |      input is still being generated.\n",
            " |  \n",
            " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
            " |      Bind config to a Runnable, returning a new Runnable.\n",
            " |  \n",
            " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
            " |  \n",
            " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
            " |  \n",
            " |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
            " |      This method is called when a class is subclassed.\n",
            " |      \n",
            " |      The default implementation does nothing. It may be\n",
            " |      overridden to extend subclasses.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.generate([\"venkat\"])"
      ],
      "metadata": {
        "id": "sHRg79QC6CD4",
        "outputId": "c97827b2-e1ab-47b2-b534-a829297ab092",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='venkat - venkat - venkat - venkat -', generation_info=None)]], llm_output=None, run=[RunInfo(run_id=UUID('bcf3f0d2-6d83-4101-a706-581f491fe08d'))])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.get_num_tokens(\"venkat\")"
      ],
      "metadata": {
        "id": "RuGoa-UU8tXm",
        "outputId": "0a97661b-e369-40a4-cfaa-155accea7062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema/language_model.py\u001b[0m in \u001b[0;36mget_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2TokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-a1c96569cb16>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"venkat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema/language_model.py\u001b[0m in \u001b[0;36mget_num_tokens\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \"\"\"\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_num_tokens_from_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema/language_model.py\u001b[0m in \u001b[0;36mget_token_ids\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \"\"\"\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_token_ids_default_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_num_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema/language_model.py\u001b[0m in \u001b[0;36m_get_token_ids_default_method\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m\"\"\"Encode the text into token IDs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# get the cached tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# tokenize the text using the GPT-2 tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema/language_model.py\u001b[0m in \u001b[0;36mget_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2TokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;34m\"Could not import transformers python package. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;34m\"This is needed in order to calculate get_token_ids. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.predict(\"venkat\")"
      ],
      "metadata": {
        "id": "TH90trU38GVx",
        "outputId": "4fd00ad0-3e57-45cd-d3fe-484f6e101f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'venkat - venkat - venkat - venkat -'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GrWh0nyG4_C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sv9rSudmAj_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM-assisted RAG evaluation prompt"
      ],
      "metadata": {
        "id": "GEgRUnv1BNL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template= \"\"\"Please act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided question based on a provided context.\n",
        "\n",
        "And you'll need to submit your grading for the correctness, comprehensiveness and readability of the answer, using the following format:\n",
        "Reasoning for correctness: [your one line step by step reasoning about the correctness of the answer]\n",
        "Score for correctness: [your score number for the correctness of the answer]\n",
        "\n",
        "\n",
        "\n",
        "  Below is your grading rubric:\n",
        "\n",
        "- Correctness: If the answer correctly answer the question, below are the details for different scores:\n",
        "  - Score 0: the answer is completely incorrect, doesn’t mention anything about the question or is completely contrary to the correct answer.\n",
        "      - For example, when asked “How to terminate a databricks cluster”, the answer is empty string, or content that’s completely irrelevant, or sorry I don’t know the answer.\n",
        "  - Score 4: the answer provides some relevance to the question and answer one aspect of the question correctly.\n",
        "      - Example:\n",
        "          - Question: How to terminate a databricks cluster\n",
        "          - Answer: Databricks cluster is a cloud-based computing environment that allows users to process big data and run distributed data processing tasks efficiently.\n",
        "          - Or answer:  In the Databricks workspace, navigate to the \"Clusters\" tab. And then this is a hard question that I need to think more about it\n",
        "  - Score 7: the answer mostly answer the question but is missing or hallucinating on one critical aspect.\n",
        "      - Example:\n",
        "          - Question: How to terminate a databricks cluster”\n",
        "          - Answer: “In the Databricks workspace, navigate to the \"Clusters\" tab.\n",
        "          Find the cluster you want to terminate from the list of active clusters.\n",
        "          And then you’ll find a button to terminate all clusters at once”\n",
        "  - Score 10: the answer correctly answer the question and not missing any major aspect\n",
        "      - Example:\n",
        "          - Question: How to terminate a databricks cluster\n",
        "          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\n",
        "          Find the cluster you want to terminate from the list of active clusters.\n",
        "          Click on the down-arrow next to the cluster name to open the cluster details.\n",
        "          Click on the \"Terminate\" button. A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.”\n",
        "\n",
        "Provided question:\n",
        "{question}\n",
        "\n",
        "Provided answer:\n",
        "{answer}\n",
        "\n",
        "Provided context:\n",
        "{context}\n",
        "\n",
        "Please provide your grading for the correctness\"\"\"\n",
        ""
      ],
      "metadata": {
        "id": "v5BgZNqFBPwO"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\",\"answer\",\"context\"],\n",
        "    template=template\n",
        ")"
      ],
      "metadata": {
        "id": "IL7oioHmDreW"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\n",
        "    eval_prompt_template.format(\n",
        "        question=\"what is the best way to improve memory performance?\",\n",
        "        answer=\"memory usage of UNIX system depends on number of users\",\n",
        "        context=\"The entire dataset has to fit in memory, consideration of memory used by your objects is the must.By having an increased high turnover of objects, the overhead of garbage collection becomes a necessity.You’ll have to take into account the cost of accessing those objects.\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "GLiclpd8Ep9s",
        "outputId": "c6f4228e-28f8-4ee9-f867-4bbd0af4becf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Instead of using strings for keys you could use numeric IDs and enumerated objects"
      ],
      "metadata": {
        "id": "3Qd2URxYFE57"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}